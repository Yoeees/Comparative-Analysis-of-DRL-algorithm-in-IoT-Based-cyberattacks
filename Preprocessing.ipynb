{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73730f09",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from collections import Counter\n",
    "\n",
    "# Set random seed for reproducibility across all random operations (e.g., labeled anomaly sampling)\n",
    "np.random.seed(42)\n",
    "\n",
    "# =============================\n",
    "# 1. Load dataset\n",
    "# =============================\n",
    "# Load the raw EdgeIIoTset CSV file containing packet-level IoT network traffic and labels\n",
    "print(\"ðŸ“‚ Loading dataset...\")\n",
    "df = pd.read_csv(\"EdgeIIoT-dataset.csv\")  \n",
    "\n",
    "# Drop columns that contain identifiers, payloads, or high-cardinality text unlikely to generalize\n",
    "# These are removed to reduce noise, prevent overfitting, and focus on behavioral features (Section 3.5.1 Data Cleaning)\n",
    "drop_cols = [\n",
    "    \"ip.src_host\", \"ip.dst_host\", \"arp.dst.proto_ipv4\", \"arp.src.proto_ipv4\",\n",
    "    \"http.file_data\", \"http.request.uri.query\", \"http.referer\",\n",
    "    \"http.request.full_uri\", \"tcp.options\", \"tcp.payload\",\n",
    "    \"dns.qry.name\", \"dns.qry.name.len\", \"mqtt.msg\"\n",
    "]\n",
    "df = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "# =============================\n",
    "# 2. Separate labels AND keep attack types\n",
    "# =============================\n",
    "# Preserve binary (Attack_label: 0=normal, 1=anomalous) and multi-class (Attack_type) labels before dropping\n",
    "# This prevents label leakage into feature space while keeping them for window-level labeling and analysis\n",
    "labels = df[[\"Attack_label\", \"Attack_type\"]].copy()\n",
    "attack_types_original = df[\"Attack_type\"].values  # Preserve original string attack types for per-window tracking\n",
    "df = df.drop(columns=[\"Attack_label\", \"Attack_type\"], errors=\"ignore\")\n",
    "\n",
    "# =============================\n",
    "# 3. Encode categorical columns\n",
    "# =============================\n",
    "# Handle categorical features (e.g., protocol types) to make them numerical\n",
    "# Low-cardinality (<50 unique): Label encoding\n",
    "# High-cardinality: Frequency encoding to capture occurrence patterns without exploding dimensions\n",
    "categorical_cols = df.select_dtypes(include=[\"object\"]).columns.drop(\"frame.time\", errors=\"ignore\")\n",
    "for col in categorical_cols:\n",
    "    n_unique = df[col].nunique()\n",
    "    if n_unique < 50:\n",
    "        # LabelEncoder for low-cardinality categorical features\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "    else:\n",
    "        # Frequency encoding for high-cardinality features (common in network data)\n",
    "        freq = df[col].value_counts()\n",
    "        df[col] = df[col].map(freq)\n",
    "\n",
    "# =============================\n",
    "# 4. Scale numerical features\n",
    "# =============================\n",
    "# Standardize all features (zero mean, unit variance) for stable neural network training (ConvAE and RL state embeddings)\n",
    "scaler = StandardScaler()\n",
    "features = df.drop(columns=[\"frame.time\"], errors=\"ignore\")  # Exclude timestamp from scaling\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Reconstruct DataFrame with scaled features and temporarily re-attach labels/timestamp for sorting\n",
    "X = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "X[\"Attack_label\"] = labels[\"Attack_label\"].values\n",
    "X[\"Attack_type\"] = labels[\"Attack_type\"].values\n",
    "if \"frame.time\" in df.columns:\n",
    "    X[\"frame.time\"] = df[\"frame.time\"].values\n",
    "\n",
    "# Sort chronologically by frame.time â€“ critical for preserving temporal sequence in streaming-like data (Section 3.5.3)\n",
    "if \"frame.time\" in X.columns:\n",
    "    X = X.sort_values(\"frame.time\").reset_index(drop=True)\n",
    "    # Re-align preserved attack types after sorting\n",
    "    attack_types_original = X[\"Attack_type\"].values\n",
    "\n",
    "print(f\"âœ… Preprocessed features: {X.shape[1]} columns (including temporary labels and timestamp)\")\n",
    "\n",
    "# =============================\n",
    "# 5. Sliding window (stride = 4) WITH ATTACK TYPE TRACKING\n",
    "# =============================\n",
    "def sliding_window_with_attack_types(data, binary_labels, attack_types,\n",
    "                                      window=32, step=4, anomaly_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Generate overlapping sliding windows for time-series modeling \n",
    "    - Fixed window size 32 captures short-term dependencies in bursty IoT attacks.\n",
    "    - Stride 4 provides dense coverage with high overlap for streaming simulation.\n",
    "    - Window labeled anomalous if â‰¥30% packets are anomalous (proportional threshold heuristic).\n",
    "    - Tracks most prevalent attack type per window for detailed analysis \n",
    "    \"\"\"\n",
    "    X_seq, y_seq, attack_type_seq = [], [], []\n",
    "   \n",
    "    for start in range(0, len(data) - window, step):\n",
    "        end = start + window\n",
    "        \n",
    "        # Extract feature window (32 timesteps Ã— features)\n",
    "        X_seq.append(data.iloc[start:end].values)\n",
    "       \n",
    "        # Compute anomaly proportion using original packet-level binary labels\n",
    "        window_labels = binary_labels.iloc[start:end]\n",
    "        anomaly_ratio = window_labels.sum() / len(window_labels)\n",
    "       \n",
    "        # Apply 30% threshold: balances sensitivity to partial attacks vs. robustness to noise \n",
    "        y_seq.append(int(anomaly_ratio >= anomaly_threshold))\n",
    "       \n",
    "        # Track dominant attack type for per-attack performance evaluation\n",
    "        window_attack_types = attack_types[start:end]\n",
    "        most_common_attack = Counter(window_attack_types).most_common(1)[0][0]\n",
    "        attack_type_seq.append(most_common_attack)\n",
    "   \n",
    "    return (np.array(X_seq, dtype=np.float32),\n",
    "            np.array(y_seq, dtype=np.int8),\n",
    "            np.array(attack_type_seq))\n",
    "\n",
    "# Generate windows â€“\n",
    "print(\"ðŸªŸ Generating sliding windows (window=32, step=4, â‰¥30% anomalies required for anomalous label)...\")\n",
    "X_seq, y_seq, attack_type_seq = sliding_window_with_attack_types(\n",
    "    X.drop(columns=[\"Attack_label\", \"Attack_type\", \"frame.time\"], errors=\"ignore\"),  # Clean features only\n",
    "    X[\"Attack_label\"],                                                                 # Packet-level binary labels\n",
    "    attack_types_original,                                                             # Original attack type strings\n",
    "    window=32,\n",
    "    step=4,\n",
    "    anomaly_threshold=0.3  # 30% proportional threshold (Section 3.5.2)\n",
    ")\n",
    "\n",
    "print(f\"âœ… Total windows: {len(X_seq):,}\")\n",
    "print(f\"âœ… Window shape: {X_seq.shape[1:]} (time steps Ã— features)\")\n",
    "print(f\"âœ… Attack types tracked: {len(np.unique(attack_type_seq))} unique types\")\n",
    "\n",
    "# Display distribution of dominant attack types across windows\n",
    "print(\"\\nðŸ“Š Attack type distribution in all windows (top 10):\")\n",
    "attack_counts = Counter(attack_type_seq)\n",
    "for attack_type, count in attack_counts.most_common(10):\n",
    "    pct = count / len(attack_type_seq) * 100\n",
    "    print(f\" {attack_type:30s}: {count:8,} ({pct:5.2f}%)\")\n",
    "if len(attack_counts) > 10:\n",
    "    print(f\" ... and {len(attack_counts) - 10} more attack types\")\n",
    "\n",
    "# =============================\n",
    "# 6. Chronological Train/Test Split (80/20, time-based)\n",
    "# =============================\n",
    "# Time-based split preserves real-world deployment scenario (earlier data for training, later for testing)\n",
    "split_idx = int(0.8 * len(X_seq))\n",
    "X_train_seq = X_seq[:split_idx]\n",
    "y_train_seq = y_seq[:split_idx]\n",
    "attack_types_train = attack_type_seq[:split_idx]\n",
    "\n",
    "X_test = X_seq[split_idx:]\n",
    "y_test = y_seq[split_idx:]\n",
    "attack_types_test = attack_type_seq[split_idx:]\n",
    "\n",
    "# Semi-supervised setup: 5% of anomalous windows fully labeled; rest unlabeled \n",
    "anomaly_indices = np.where(y_train_seq == 1)[0]\n",
    "num_labeled = int(0.05 * len(anomaly_indices))  # 5% of training anomalies are labeled\n",
    "labeled_anom_idx = np.random.choice(anomaly_indices, num_labeled, replace=False)\n",
    "\n",
    "X_labeled = X_train_seq[labeled_anom_idx]\n",
    "y_labeled = y_train_seq[labeled_anom_idx]\n",
    "\n",
    "unlabeled_idx = np.setdiff1d(np.arange(len(X_train_seq)), labeled_anom_idx)\n",
    "X_unlabeled = X_train_seq[unlabeled_idx]\n",
    "y_unlabeled = y_train_seq[unlabeled_idx]  # Retained only for sanity checks; hidden during RL training\n",
    "\n",
    "# ConvAE trained exclusively on normal (non-anomalous) windows from training split\n",
    "normal_indices = np.where(y_train_seq == 0)[0]\n",
    "X_ae = X_train_seq[normal_indices]\n",
    "\n",
    "# =============================\n",
    "# 7. Save outputs WITH INDICES AND ATTACK TYPES\n",
    "# =============================\n",
    "# Save processed arrays for ConvAE pretraining and RL training/inference\n",
    "np.save(\"X_ae.npy\", X_ae)                  # Normal sequences for ConvAE\n",
    "np.save(\"X_unlabeled.npy\", X_unlabeled)\n",
    "np.save(\"y_unlabeled.npy\", y_unlabeled)    # For verification only\n",
    "np.save(\"X_labeled.npy\", X_labeled)\n",
    "np.save(\"y_labeled.npy\", y_labeled)\n",
    "np.save(\"X_test.npy\", X_test)\n",
    "np.save(\"y_test.npy\", y_test)\n",
    "np.save(\"y_train_seq.npy\", y_train_seq)\n",
    "\n",
    "# Save attack types and indices for reproducibility and per-attack analysis \n",
    "np.save(\"attack_types_test.npy\", attack_types_test)\n",
    "np.save(\"attack_types_train.npy\", attack_types_train)\n",
    "np.save(\"labeled_indices.npy\", labeled_anom_idx)\n",
    "np.save(\"unlabeled_indices.npy\", unlabeled_idx)\n",
    "np.save(\"train_split_size.npy\", np.array([len(X_train_seq)]))\n",
    "\n",
    "print(\"\\nðŸ’¾ Saved arrays (sizes in GB where applicable):\")\n",
    "print(f\" X_ae          : {X_ae.shape} ({X_ae.nbytes / 1e9:.2f} GB)\")\n",
    "print(f\" X_unlabeled   : {X_unlabeled.shape} ({X_unlabeled.nbytes / 1e9:.2f} GB)\")\n",
    "print(f\" X_labeled     : {X_labeled.shape} ({X_labeled.nbytes / 1e9:.2f} GB)\")\n",
    "print(f\" X_test        : {X_test.shape} ({X_test.nbytes / 1e9:.2f} GB)\")\n",
    "\n",
    "# =============================\n",
    "# 8. Dataset Statistics\n",
    "# =============================\n",
    "# Comprehensive statistics for transparency and verification (aligns with thesis reporting)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š DATASET STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_normal = np.sum(y_train_seq == 0)\n",
    "train_anomaly = np.sum(y_train_seq == 1)\n",
    "train_total = len(y_train_seq)\n",
    "print(f\"\\nðŸ”¹ TRAINING SPLIT (First 80% chronologically):\")\n",
    "print(f\" Total windows   : {train_total:,}\")\n",
    "print(f\" Normal windows  : {train_normal:,} ({train_normal/train_total*100:.2f}%)\")\n",
    "print(f\" Anomaly windows : {train_anomaly:,} ({train_anomaly/train_total*100:.2f}%)\")\n",
    "\n",
    "test_normal = np.sum(y_test == 0)\n",
    "test_anomaly = np.sum(y_test == 1)\n",
    "test_total = len(y_test)\n",
    "print(f\"\\nðŸ”¹ TEST SPLIT (Last 20% chronologically):\")\n",
    "print(f\" Total windows   : {test_total:,}\")\n",
    "print(f\" Normal windows  : {test_normal:,} ({test_normal/test_total*100:.2f}%)\")\n",
    "print(f\" Anomaly windows : {test_anomaly:,} ({test_anomaly/test_total*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ”¹ TEST SET ATTACK TYPES (dominant per window):\")\n",
    "test_attack_counts = Counter(attack_types_test)\n",
    "for attack_type, count in test_attack_counts.most_common():\n",
    "    pct = count / len(attack_types_test) * 100\n",
    "    print(f\" {attack_type:30s}: {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ”¹ OVERALL:\")\n",
    "print(f\" Total windows   : {train_total + test_total:,}\")\n",
    "print(f\" Normal windows  : {train_normal + test_normal:,}\")\n",
    "print(f\" Anomaly windows : {train_anomaly + test_anomaly:,}\")\n",
    "print(f\" Unique attacks  : {len(np.unique(attack_type_seq))}\")\n",
    "\n",
    "print(f\"\\nðŸ”¹ LABELED ANOMALIES (for RL training):\")\n",
    "print(f\" Labeled anomalies : {len(X_labeled):,} ({len(X_labeled)/train_anomaly*100:.2f}% of train anomalies)\")\n",
    "print(f\" Unlabeled data    : {len(X_unlabeled):,}\")\n",
    "print(f\" ConvAE data       : {len(X_ae):,} (normal sequences only)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"âœ… Preprocessing complete! Ready for ConvAE pretraining and DRL training.\")\n",
    "print(f\"âœ… Index files saved for full reproducibility.\")\n",
    "print(f\"âœ… Attack type information preserved for per-attack analysis.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

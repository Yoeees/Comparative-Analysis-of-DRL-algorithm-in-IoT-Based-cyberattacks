{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23621b80",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Load pretrained ConvAE (frozen) and AE stats\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ae_model = ConvAE(feat_dim=X_unlabeled.shape[2], seq_len=X_unlabeled.shape[1], latent_dim=128)\n",
    "ae_model.load_state_dict(torch.load(\"convAE_best.pth\", map_location=device))\n",
    "ae_model.to(device)\n",
    "ae_model.eval()\n",
    "for p in ae_model.parameters():\n",
    "    p.requires_grad = False\n",
    "print(f\"‚úÖ Loaded ConvAE on {device} (frozen for Standard PPO).\")\n",
    "\n",
    "stats = np.load(\"convAE_stats.npz\")\n",
    "ae_mean, ae_std = stats[\"mean_err\"], stats[\"std_err\"]\n",
    "print(f\"‚úÖ AE normalization loaded: mean={ae_mean:.6f}, std={ae_std:.6f}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Reconstruct chronological order\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîÑ RECONSTRUCTING CHRONOLOGICAL ORDER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load labeled and unlabeled data\n",
    "X_labeled = np.load(\"X_labeled.npy\")\n",
    "y_labeled = np.load(\"y_labeled.npy\")\n",
    "X_unlabeled = np.load(\"X_unlabeled.npy\")\n",
    "y_unlabeled = np.load(\"y_unlabeled.npy\")\n",
    "\n",
    "# Load indices\n",
    "labeled_indices = np.load(\"labeled_indices.npy\")\n",
    "unlabeled_indices = np.load(\"unlabeled_indices.npy\")\n",
    "train_split_size = np.load(\"train_split_size.npy\")[0]\n",
    "\n",
    "print(f\"üìä Data loaded:\")\n",
    "print(f\"   Labeled samples: {len(X_labeled):,}\")\n",
    "print(f\"   Unlabeled samples: {len(X_unlabeled):,}\")\n",
    "print(f\"   Original training size: {train_split_size:,}\")\n",
    "\n",
    "# Reconstruct X and y in chronological order\n",
    "X_train_reconstructed = np.zeros((train_split_size, *X_labeled.shape[1:]), dtype=X_labeled.dtype)\n",
    "y_train_reconstructed = np.zeros(train_split_size, dtype=y_labeled.dtype)\n",
    "\n",
    "# Place data back at original positions\n",
    "X_train_reconstructed[labeled_indices] = X_labeled\n",
    "y_train_reconstructed[labeled_indices] = y_labeled\n",
    "X_train_reconstructed[unlabeled_indices] = X_unlabeled\n",
    "y_train_reconstructed[unlabeled_indices] = y_unlabeled\n",
    "\n",
    "# Create supervision mask (1 = labeled/supervised, 0 = unlabeled/unsupervised)\n",
    "supervision_mask = np.zeros(train_split_size, dtype=np.int8)\n",
    "supervision_mask[labeled_indices] = 1\n",
    "\n",
    "print(f\"\\n‚úÖ Reconstructed training data:\")\n",
    "print(f\"   Shape: {X_train_reconstructed.shape}\")\n",
    "print(f\"   Supervised positions: {np.sum(supervision_mask):,} ({np.sum(supervision_mask)/train_split_size*100:.2f}%)\")\n",
    "print(f\"   Unsupervised positions: {np.sum(1-supervision_mask):,} ({np.sum(1-supervision_mask)/train_split_size*100:.2f}%)\")\n",
    "\n",
    "# Verify reconstruction\n",
    "y_train_original = np.load(\"y_train_seq.npy\")\n",
    "if np.array_equal(y_train_reconstructed, y_train_original):\n",
    "    print(\"‚úÖ VERIFICATION PASSED: Reconstruction matches original!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Reconstruction mismatch detected!\")\n",
    "\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: PPO Environment (Chronological Sequential)\n",
    "# ============================================================================\n",
    "\n",
    "class PPOAEEnvChronological(gym.Env):\n",
    "    \"\"\"\n",
    "    Sequential environment that processes windows in chronological order.\n",
    "    Uses supervision_mask to determine which positions have labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_train, y_train, supervision_mask, ae_model,\n",
    "                 embeddings, lambda_int=1.0, max_steps=2000):\n",
    "        super().__init__()\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.supervision_mask = supervision_mask\n",
    "        self.ae_model = ae_model\n",
    "        self.device = next(ae_model.parameters()).device\n",
    "        self.lambda_int = lambda_int\n",
    "        self.max_steps = max_steps\n",
    "        self.steps = 0\n",
    "        self.idx = 0\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "        emb_dim = self.embeddings.shape[1]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, \n",
    "            shape=(emb_dim + 1,), \n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.action_space = spaces.Discrete(2)  # 0 = normal, 1 = anomaly\n",
    "\n",
    "        # Statistics\n",
    "        n_supervised = np.sum(supervision_mask)\n",
    "        n_unsupervised = len(supervision_mask) - n_supervised\n",
    "        print(f\"\\nüîπ Environment initialized:\")\n",
    "        print(f\"   Total windows: {len(X_train):,}\")\n",
    "        print(f\"   Supervised: {n_supervised:,} ({n_supervised/len(X_train)*100:.2f}%)\")\n",
    "        print(f\"   Unsupervised: {n_unsupervised:,} ({n_unsupervised/len(X_train)*100:.2f}%)\")\n",
    "\n",
    "    def _ae_error(self, x):\n",
    "        \"\"\"Compute normalized reconstruction error.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(x, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "            recon = self.ae_model(x)\n",
    "            loss = F.mse_loss(recon, x, reduction=\"mean\").item()\n",
    "        norm = np.tanh((loss - ae_mean) / (ae_std + 1e-8))\n",
    "        return max(0, norm)\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        \"\"\"Reset to start of sequence (or random position 10% of time).\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        self.steps = 0\n",
    "        \n",
    "        # Start from beginning or random position\n",
    "        if np.random.rand() < 0.1:  # 10% random start for variety\n",
    "            self.idx = np.random.randint(0, len(self.X_train))\n",
    "        else:\n",
    "            self.idx = 0\n",
    "        \n",
    "        emb = self.embeddings[self.idx]\n",
    "        err = np.array([self._ae_error(self.X_train[self.idx])])\n",
    "        obs = np.concatenate([emb, err]).astype(np.float32)\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take action on current window, move to next chronologically.\"\"\"\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Current window\n",
    "        x = self.X_train[self.idx]\n",
    "        true_label = self.y_train[self.idx]\n",
    "        is_supervised = self.supervision_mask[self.idx]\n",
    "        \n",
    "        # Compute reconstruction error\n",
    "        err = self._ae_error(x)\n",
    "        \n",
    "        # ========== REWARD CALCULATION ==========\n",
    "        \n",
    "        # External reward (only for supervised positions)\n",
    "        if is_supervised == 1:\n",
    "            # We have ground truth label\n",
    "            external_reward = 1.0 if action == true_label else -0.5\n",
    "        else:\n",
    "            # No supervision - no external reward\n",
    "            external_reward = 0.0\n",
    "        \n",
    "        # Intrinsic reward (based on reconstruction error)\n",
    "        # Encourage anomaly prediction when error is high\n",
    "        if action == 1:  # Predicted anomaly\n",
    "            intrinsic_reward = self.lambda_int * err\n",
    "            # Penalize weak false positives\n",
    "            if err < 0.05:\n",
    "                intrinsic_reward -= 0.5\n",
    "        else:  # Predicted normal\n",
    "            intrinsic_reward = 0.0\n",
    "        \n",
    "        # Total reward\n",
    "        reward = external_reward + intrinsic_reward\n",
    "        reward = np.clip(reward, -5, 5)\n",
    "        \n",
    "        # ========== MOVE TO NEXT WINDOW (CHRONOLOGICALLY) ==========\n",
    "        self.idx = (self.idx + 1) % len(self.X_train)\n",
    "        \n",
    "        # Next observation\n",
    "        next_emb = self.embeddings[self.idx]\n",
    "        next_err = np.array([self._ae_error(self.X_train[self.idx])])\n",
    "        obs = np.concatenate([next_emb, next_err]).astype(np.float32)\n",
    "        \n",
    "        done = self.steps >= self.max_steps\n",
    "        \n",
    "        return obs, reward, done, False, {}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Precompute embeddings\n",
    "# ============================================================================\n",
    "\n",
    "def compute_embeddings(X):\n",
    "    \"\"\"Compute embeddings for all windows.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        tensors = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "        embeds = ae_model.encode(tensors).cpu().numpy()\n",
    "    return embeds\n",
    "\n",
    "print(\"üîπ Precomputing embeddings for reconstructed data...\")\n",
    "embeddings_train = compute_embeddings(X_train_reconstructed)\n",
    "print(f\"‚úÖ Embeddings computed: {embeddings_train.shape}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Create VecEnv + Standard PPO\n",
    "# ============================================================================\n",
    "\n",
    "def make_env(rank):\n",
    "    def _init():\n",
    "        env = PPOAEEnvChronological(\n",
    "            X_train_reconstructed, \n",
    "            y_train_reconstructed, \n",
    "            supervision_mask,\n",
    "            ae_model,\n",
    "            embeddings_train,\n",
    "            lambda_int=0.8, \n",
    "            max_steps=2000\n",
    "        )\n",
    "        return Monitor(env, f\"logs/env_{rank}\")\n",
    "    return _init\n",
    "\n",
    "num_envs = 1\n",
    "vec_env = DummyVecEnv([make_env(i) for i in range(num_envs)])\n",
    "vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
    "\n",
    "# Standard MLP policy (no LSTM)\n",
    "policy_kwargs = dict(\n",
    "    net_arch=dict(pi=[256, 256], vf=[512, 512, 256])  # Same architecture as recurrent, but no LSTM\n",
    ")\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",  # Standard MLP policy (non-recurrent)\n",
    "    vec_env,\n",
    "    verbose=1,\n",
    "    learning_rate=1e-4,\n",
    "    n_steps=128,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    ent_coef=0.02,\n",
    "    clip_range=0.2,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    tensorboard_log=\"logs_standard_ppo_ae_chronological/\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ Starting Standard PPO training with chronological data\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Train Standard PPO\n",
    "# ============================================================================\n",
    "\n",
    "model.learn(total_timesteps=500_000)\n",
    "model.save(\"standard_ppo_ae_chronological\")\n",
    "vec_env.save(\"vec_normalize_standard_ppo_chronological.pkl\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete! Chronological Standard PPO model saved.\")\n",
    "print(\"   Model: standard_ppo_ae_chronological.zip\")\n",
    "print(\"   VecNormalize: vec_normalize_standard_ppo_chronological.pkl\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

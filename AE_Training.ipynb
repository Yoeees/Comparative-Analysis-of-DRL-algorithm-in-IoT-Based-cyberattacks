{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20645,
     "status": "ok",
     "timestamp": 1769901913818,
     "user": {
      "displayName": "Johnwences Dumangcas",
      "userId": "02779738527886774707"
     },
     "user_tz": -480
    },
    "id": "r6HzXsi1EkMD",
    "outputId": "a521c9ce-48a9-442d-c8f1-119beede12d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/Thesis-2/THESIS-1\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/Thesis-2/THESIS-1\n",
    "import sys\n",
    "sys.path.append(\"/content/drive/MyDrive/Thesis-2/THESIS-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acCSdBmjO10Q"
   },
   "source": [
    "# Convolutional Autoencoder (ConvAE) Training\n",
    "\n",
    "Trains a convolutional autoencoder exclusively on **normal network traffic sequences** to learn compressed representations of benign patterns. The model learns to reconstruct normal data with low error, making it sensitive to anomalies which produce higher reconstruction errors. This reconstruction error becomes the intrinsic reward signal for the PPO agent.\n",
    "\n",
    "The training pipeline includes validation-based early stopping, learning rate scheduling, and continuous evaluation against a sample of known anomalies to monitor discrimination capability. The model outputs normalized error statistics (mean, std) used by the PPO environment to scale intrinsic rewards consistently.\n",
    "\n",
    "## Training Configuration\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Input shape | (32, features) - sequential windows |\n",
    "| Latent dimension | 128-dim compressed representation |\n",
    "| Optimizer | Adam (lr=1e-3, weight_decay=1e-5) |\n",
    "| Loss function | MSE (Mean Squared Error) |\n",
    "| Batch size | 128 |\n",
    "| Max epochs | 100 |\n",
    "| Early stopping patience | 5 epochs |\n",
    "| LR scheduler | ReduceLROnPlateau (patience=10, factor=0.5) |\n",
    "\n",
    "## Evaluation Metrics (During Training)\n",
    "\n",
    "For each epoch, the model tracks:\n",
    "- **Train Loss**: Average MSE on normal training data\n",
    "- **Val Loss**: Average MSE on held-out normal validation data\n",
    "- **Anomaly Error**: Mean reconstruction error on 1,000 labeled anomalies\n",
    "- **Ratio**: (Anomaly Error / Normal Error) - measures separation capability\n",
    "- **AUROC**: Area under ROC curve comparing normal vs. anomaly reconstruction errors\n",
    "\n",
    "Higher ratio and AUROC values indicate better anomaly discrimination.\n",
    "\n",
    "## Output Files\n",
    "\n",
    "| File | Purpose |\n",
    "|------|---------|\n",
    "| `convAE_best.pth` | Best model weights (lowest validation loss) |\n",
    "| `convAE_final.pth` | Final model weights after training completes |\n",
    "| `convAE_stats.npz` | Normalization statistics (`mean_err`, `std_err`) for PPO environment |\n",
    "\n",
    "## Workflow\n",
    "```\n",
    "Normal sequences (X_ae.npy)\n",
    "         â†“\n",
    "[80/20 Train/Val Split]\n",
    "         â†“\n",
    "[ConvAE Training Loop]\n",
    "  â€¢ Forward pass: encode â†’ decode\n",
    "  â€¢ MSE loss on reconstruction\n",
    "  â€¢ Backprop + optimizer step\n",
    "         â†“\n",
    "[Validation + Early Stopping]\n",
    "  â€¢ Track val loss\n",
    "  â€¢ Save best model\n",
    "  â€¢ Compute error on anomalies\n",
    "         â†“\n",
    "[Compute Stats on Full Dataset]\n",
    "  â€¢ Mean reconstruction error\n",
    "  â€¢ Std reconstruction error\n",
    "         â†“\n",
    "Save: convAE_best.pth + convAE_stats.npz\n",
    "```\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Anomaly-Free Training**: Only normal sequences ensure the model learns \"normal\" patterns exclusively\n",
    "- **Early Stopping**: Prevents overfitting by monitoring validation loss\n",
    "- **Anomaly Evaluation**: Tracks separation between normal and anomalous reconstruction errors during training\n",
    "- **Normalization Stats**: Computes mean/std for consistent error scaling in PPO intrinsic rewards\n",
    "\n",
    "**Status:** Complete ConvAE training pipeline ready for PPO integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 859
    },
    "executionInfo": {
     "elapsed": 228368,
     "status": "ok",
     "timestamp": 1769902142204,
     "user": {
      "displayName": "Johnwences Dumangcas",
      "userId": "02779738527886774707"
     },
     "user_tz": -480
    },
    "id": "oSsQIpXAFGBa",
    "outputId": "66935dd2-5d7c-48f5-f328-116ce93e8102"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from convAE import ConvAE\n",
    "\n",
    "# ============================================================\n",
    "# 1. Load Data\n",
    "# ============================================================\n",
    "print(\"ðŸ”¹ Loading preprocessed AE dataset...\")\n",
    "X_normals = np.load(\"X_ae.npy\").astype(np.float32)\n",
    "print(f\"Loaded ConvAE training set: {X_normals.shape} (pure normals)\")\n",
    "\n",
    "# Load labeled anomalies for evaluation\n",
    "try:\n",
    "    X_labeled = np.load(\"X_labeled.npy\").astype(np.float32)\n",
    "    K = min(1000, len(X_labeled))\n",
    "    X_labeled_sample = X_labeled[:K]\n",
    "    print(f\"Loaded {K} labeled anomaly samples for AE evaluation.\")\n",
    "except Exception as e:\n",
    "    print(\" No labeled anomalies found for AE evaluation. Skipping anomaly metrics.\")\n",
    "    X_labeled_sample = None\n",
    "\n",
    "# ============================================================\n",
    "# 2. Dataset + Loaders\n",
    "# ============================================================\n",
    "dataset = TensorDataset(torch.tensor(X_normals))\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=128, shuffle=False)\n",
    "\n",
    "# ============================================================\n",
    "# 3. Model Setup\n",
    "# ============================================================\n",
    "seq_len, feat_dim = X_normals.shape[1], X_normals.shape[2]\n",
    "ae_model = ConvAE(feat_dim=feat_dim, seq_len=seq_len, latent_dim=128)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ae_model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(ae_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
    "\n",
    "# ============================================================\n",
    "# 4. Training Loop\n",
    "# ============================================================\n",
    "epochs = 100\n",
    "train_losses, val_losses = [], []\n",
    "best_val_loss = float(\"inf\")\n",
    "patience, patience_counter = 5, 0\n",
    "\n",
    "print(\"Starting ConvAE training...\\n\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ae_model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        x = batch[0].to(device)\n",
    "        recon = ae_model(x)\n",
    "        loss = criterion(recon, x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # --- Validation ---\n",
    "    ae_model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_errs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x = batch[0].to(device)\n",
    "            recon = ae_model(x)\n",
    "            val_loss = criterion(recon, x).item()\n",
    "            total_val_loss += val_loss\n",
    "            per_sample = F.mse_loss(recon, x, reduction=\"none\").mean(dim=(1, 2))\n",
    "            val_errs.append(per_sample.cpu().numpy())\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(ae_model.state_dict(), \"convAE_best.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"â¹ï¸ Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # --- Optional: Anomaly Evaluation ---\n",
    "    if X_labeled_sample is not None:\n",
    "        with torch.no_grad():\n",
    "            val_errs = np.concatenate(val_errs)\n",
    "            mean_val_err = val_errs.mean()\n",
    "\n",
    "            anom_batch = torch.tensor(X_labeled_sample, dtype=torch.float32).to(device)\n",
    "            recon_anom = ae_model(anom_batch)\n",
    "            per_sample_err_anom = F.mse_loss(recon_anom, anom_batch, reduction=\"none\").mean(dim=(1, 2)).cpu().numpy()\n",
    "            mean_anom_err = per_sample_err_anom.mean()\n",
    "\n",
    "            ratio = mean_anom_err / mean_val_err if mean_val_err > 0 else float(\"inf\")\n",
    "            M = min(2000, len(val_errs), len(per_sample_err_anom))\n",
    "            if M > 0:\n",
    "                y_true = np.concatenate([np.zeros(M), np.ones(M)])\n",
    "                scores = np.concatenate([val_errs[:M], per_sample_err_anom[:M]])\n",
    "                auc = roc_auc_score(y_true, scores)\n",
    "            else:\n",
    "                auc = float(\"nan\")\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1:03d}/{epochs} | \"\n",
    "                  f\"Train: {avg_train_loss:.6f} | Val: {avg_val_loss:.6f} | \"\n",
    "                  f\"AnomErr: {mean_anom_err:.6f} | Ratio: {ratio:.2f}Ã— | AUROC: {auc:.4f}\")\n",
    "    else:\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1:03d}/{epochs} | \"\n",
    "                  f\"Train: {avg_train_loss:.6f} | Val: {avg_val_loss:.6f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 5. Post-Training\n",
    "# ============================================================\n",
    "print(\"\\nâœ… Training complete! Best model saved as convAE_best.pth\")\n",
    "\n",
    "torch.save(ae_model.state_dict(), \"convAE_final.pth\")\n",
    "\n",
    "# Compute normalization stats for PPOEnv\n",
    "with torch.no_grad():\n",
    "    ae_model.eval()\n",
    "    val_loader_full = DataLoader(dataset, batch_size=128, shuffle=False)\n",
    "    all_errs = []\n",
    "    for batch in val_loader_full:\n",
    "        x = batch[0].to(device)\n",
    "        recon = ae_model(x)\n",
    "        per_sample = F.mse_loss(recon, x, reduction=\"none\").mean(dim=(1, 2))\n",
    "        all_errs.append(per_sample.cpu().numpy())\n",
    "    all_errs = np.concatenate(all_errs)\n",
    "    mean_err, std_err = float(all_errs.mean()), float(all_errs.std())\n",
    "\n",
    "np.savez(\"convAE_stats.npz\", mean_err=mean_err, std_err=std_err)\n",
    "print(f\"ðŸ“Š Saved AE stats for PPOEnv: mean={mean_err:.6f}, std={std_err:.6f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 6. Plot Training Curves\n",
    "# ============================================================\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1, len(train_losses)+1), train_losses, label=\"Train\", marker=\"o\")\n",
    "plt.plot(range(1, len(val_losses)+1), val_losses, label=\"Val\", marker=\"s\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Reconstruction Error (MSE)\")\n",
    "plt.title(\"ConvAE Training on Normal Sequences\")\n",
    "plt.legend(); plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1769902142446,
     "user": {
      "displayName": "Johnwences Dumangcas",
      "userId": "02779738527886774707"
     },
     "user_tz": -480
    },
    "id": "BDkod3UrRTtQ",
    "outputId": "eb5b2bd7-d955-46df-da0d-5f933573a8a7"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. Compute Quartiles & IQR-based Confidence Threshold\n",
    "# ============================================================\n",
    "print(\"\\nðŸ” Computing quartiles and IQR threshold on normal reconstruction errors...\")\n",
    "\n",
    "\n",
    "if 'all_errs' in globals() and len(all_errs) > 0:\n",
    "    normal_mse = all_errs  # raw MSE values on normals\n",
    "else:\n",
    "    print(\"Warning: all_errs not found. Recomputing...\")\n",
    "    \n",
    "\n",
    "# Raw MSE quartiles\n",
    "Q1 = np.percentile(normal_mse, 25)\n",
    "Q2 = np.percentile(normal_mse, 50)\n",
    "Q3 = np.percentile(normal_mse, 75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Thresholds\n",
    "threshold_mild = Q3 + 1.5 * IQR    # standard\n",
    "threshold_conservative = Q3 + 3.0 * IQR\n",
    "\n",
    "# Normalized (using the saved mean/std)\n",
    "normalized_mse = (normal_mse - mean_err) / std_err\n",
    "Q1_norm = np.percentile(normalized_mse, 25)\n",
    "Q3_norm = np.percentile(normalized_mse, 75)\n",
    "IQR_norm = Q3_norm - Q1_norm\n",
    "threshold_norm_mild = Q3_norm + 1.5 * IQR_norm\n",
    "\n",
    "# Basic stats: estimated false positive rate on normals\n",
    "fpr_mild_raw = np.mean(normal_mse > threshold_mild) * 100\n",
    "fpr_conservative_raw = np.mean(normal_mse > threshold_conservative) * 100\n",
    "\n",
    "print(\"\\nMSE Quartiles & Thresholds:\")\n",
    "print(f\"  Q1 (25th):     {Q1:.6f}\")\n",
    "print(f\"  Median (Q2):   {Q2:.6f}\")\n",
    "print(f\"  Q3 (75th):     {Q3:.6f}\")\n",
    "print(f\"  IQR:           {IQR:.6f}\")\n",
    "print(f\"  Threshold (k=1.5): {threshold_mild:.6f}  â†’ Est. FPR: {fpr_mild_raw:.2f}%\")\n",
    "print(f\"  Threshold (k=3.0): {threshold_conservative:.6f}  â†’ Est. FPR: {fpr_conservative_raw:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# Optional: Save thresholds for later use\n",
    "thresholds_dict = {\n",
    "    'Q1': Q1, 'Q3': Q3, 'IQR': IQR,\n",
    "    'threshold_k1_5': threshold_mild,\n",
    "    'threshold_k3_0': threshold_conservative\n",
    "}\n",
    "np.savez(\"convAE_thresholds.npz\", **thresholds_dict)\n",
    "print(\"Thresholds saved to convAE_thresholds.npz\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

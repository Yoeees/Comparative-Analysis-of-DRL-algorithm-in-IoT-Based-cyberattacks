{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dfd396",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "from gym import spaces\n",
    "from sb3_contrib import QRDQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Load pretrained ConvAE (frozen) and AE stats\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ae_model = ConvAE(feat_dim=X_unlabeled.shape[2], seq_len=X_unlabeled.shape[1], latent_dim=128)\n",
    "ae_model.load_state_dict(torch.load(\"convAE_best.pth\", map_location=device))\n",
    "ae_model.to(device)\n",
    "ae_model.eval()\n",
    "for p in ae_model.parameters():\n",
    "    p.requires_grad = False\n",
    "print(f\"‚úÖ Loaded ConvAE on {device} (frozen for QR-DQN).\")\n",
    "\n",
    "stats = np.load(\"convAE_stats.npz\")\n",
    "ae_mean, ae_std = stats[\"mean_err\"], stats[\"std_err\"]\n",
    "print(f\"‚úÖ AE normalization loaded: mean={ae_mean:.6f}, std={ae_std:.6f}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Reconstruct chronological order\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîÑ RECONSTRUCTING CHRONOLOGICAL ORDER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load labeled and unlabeled data\n",
    "X_labeled = np.load(\"X_labeled.npy\")\n",
    "y_labeled = np.load(\"y_labeled.npy\")\n",
    "X_unlabeled = np.load(\"X_unlabeled.npy\")\n",
    "y_unlabeled = np.load(\"y_unlabeled.npy\")\n",
    "\n",
    "# Load indices\n",
    "labeled_indices = np.load(\"labeled_indices.npy\")\n",
    "unlabeled_indices = np.load(\"unlabeled_indices.npy\")\n",
    "train_split_size = np.load(\"train_split_size.npy\")[0]\n",
    "\n",
    "print(f\"üìä Data loaded:\")\n",
    "print(f\"   Labeled samples: {len(X_labeled):,}\")\n",
    "print(f\"   Unlabeled samples: {len(X_unlabeled):,}\")\n",
    "print(f\"   Original training size: {train_split_size:,}\")\n",
    "\n",
    "# Reconstruct X and y in chronological order\n",
    "X_train_reconstructed = np.zeros((train_split_size, *X_labeled.shape[1:]), dtype=X_labeled.dtype)\n",
    "y_train_reconstructed = np.zeros(train_split_size, dtype=y_labeled.dtype)\n",
    "\n",
    "# Place data back at original positions\n",
    "X_train_reconstructed[labeled_indices] = X_labeled\n",
    "y_train_reconstructed[labeled_indices] = y_labeled\n",
    "X_train_reconstructed[unlabeled_indices] = X_unlabeled\n",
    "y_train_reconstructed[unlabeled_indices] = y_unlabeled\n",
    "\n",
    "# Create supervision mask (1 = labeled/supervised, 0 = unlabeled/unsupervised)\n",
    "supervision_mask = np.zeros(train_split_size, dtype=np.int8)\n",
    "supervision_mask[labeled_indices] = 1\n",
    "\n",
    "print(f\"\\n‚úÖ Reconstructed training data:\")\n",
    "print(f\"   Shape: {X_train_reconstructed.shape}\")\n",
    "print(f\"   Supervised positions: {np.sum(supervision_mask):,} ({np.sum(supervision_mask)/train_split_size*100:.2f}%)\")\n",
    "print(f\"   Unsupervised positions: {np.sum(1-supervision_mask):,} ({np.sum(1-supervision_mask)/train_split_size*100:.2f}%)\")\n",
    "\n",
    "# Verify reconstruction\n",
    "y_train_original = np.load(\"y_train_seq.npy\")\n",
    "if np.array_equal(y_train_reconstructed, y_train_original):\n",
    "    print(\"‚úÖ VERIFICATION PASSED: Reconstruction matches original!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Reconstruction mismatch detected!\")\n",
    "\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: QR-DQN Environment (Chronological Sequential)\n",
    "# ============================================================================\n",
    "\n",
    "class QRDQNAEEnvChronological(gym.Env):\n",
    "    \"\"\"\n",
    "    Sequential environment that processes windows in chronological order.\n",
    "    Uses supervision_mask to determine which positions have labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_train, y_train, supervision_mask, ae_model,\n",
    "                 embeddings, lambda_int=1.0, max_steps=2000):\n",
    "        super().__init__()\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.supervision_mask = supervision_mask\n",
    "        self.ae_model = ae_model\n",
    "        self.device = next(ae_model.parameters()).device\n",
    "        self.lambda_int = lambda_int\n",
    "        self.max_steps = max_steps\n",
    "        self.steps = 0\n",
    "        self.idx = 0\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "        self.episode_count = 0\n",
    "\n",
    "        emb_dim = self.embeddings.shape[1]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, \n",
    "            shape=(emb_dim + 1,), \n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.action_space = spaces.Discrete(2)  # 0 = normal, 1 = anomaly\n",
    "\n",
    "        # Statistics\n",
    "        n_supervised = np.sum(supervision_mask)\n",
    "        n_unsupervised = len(supervision_mask) - n_supervised\n",
    "        print(f\"\\nüîπ Environment initialized:\")\n",
    "        print(f\"   Total windows: {len(X_train):,}\")\n",
    "        print(f\"   Supervised: {n_supervised:,} ({n_supervised/len(X_train)*100:.2f}%)\")\n",
    "        print(f\"   Unsupervised: {n_unsupervised:,} ({n_unsupervised/len(X_train)*100:.2f}%)\")\n",
    "\n",
    "    def _ae_error(self, x):\n",
    "        \"\"\"Compute normalized reconstruction error.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(x, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "            recon = self.ae_model(x)\n",
    "            loss = F.mse_loss(recon, x, reduction=\"mean\").item()\n",
    "        norm = np.tanh((loss - ae_mean) / (ae_std + 1e-8))\n",
    "        return max(0, norm)\n",
    "\n",
    "     def reset(self, *, seed=45, options=None):\n",
    "        \"\"\"\n",
    "        Reset with SYSTEMATIC starting positions for guaranteed coverage.\n",
    "        Each episode starts exactly max_steps positions ahead of the previous.\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        self.steps = 0\n",
    "        \n",
    "        self.idx = (self.episode_count * self.max_steps) % len(self.X_train)\n",
    "        self.episode_count += 1\n",
    "        \n",
    "        # Log coverage progress periodically\n",
    "        if self.episode_count % 50 == 0:\n",
    "            chunks_covered = self.episode_count % (len(self.X_train) // self.max_steps)\n",
    "            if chunks_covered == 0:\n",
    "                chunks_covered = len(self.X_train) // self.max_steps\n",
    "            total_chunks = len(self.X_train) // self.max_steps\n",
    "            coverage_pct = min(100, (chunks_covered / total_chunks) * 100)\n",
    "            pass_num = (self.episode_count // total_chunks) + 1\n",
    "            print(f\"üìä Episode {self.episode_count}: Starting at idx={self.idx:,} | \"\n",
    "                  f\"Pass #{pass_num} | Coverage: {coverage_pct:.1f}%\")\n",
    "        \n",
    "        emb = self.embeddings[self.idx]\n",
    "        err = np.array([self._ae_error(self.X_train[self.idx])])\n",
    "        obs = np.concatenate([emb, err]).astype(np.float32)\n",
    "        return obs, {}\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take action on current window, move to next chronologically.\"\"\"\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Current window\n",
    "        x = self.X_train[self.idx]\n",
    "        true_label = self.y_train[self.idx]\n",
    "        is_supervised = self.supervision_mask[self.idx]\n",
    "        \n",
    "        # Compute reconstruction error\n",
    "        err = self._ae_error(x)\n",
    "        \n",
    "        # ========== REWARD CALCULATION ==========\n",
    "        \n",
    "        # External reward (only for supervised positions)\n",
    "        if is_supervised == 1:\n",
    "            # We have ground truth label\n",
    "            external_reward = 1.0 if action == true_label else -0.5\n",
    "        else:\n",
    "            # No supervision - no external reward\n",
    "            external_reward = 0.0\n",
    "        \n",
    "        # Intrinsic reward (based on reconstruction error)\n",
    "        # Encourage anomaly prediction when error is high\n",
    "        if action == 1:  # Predicted anomaly\n",
    "            intrinsic_reward = self.lambda_int * err\n",
    "            # Penalize weak false positives\n",
    "            if err < 0.09:\n",
    "                intrinsic_reward -= 0.5\n",
    "        else:  # Predicted normal\n",
    "            intrinsic_reward = 0.0\n",
    "        \n",
    "        # Total reward\n",
    "        reward = external_reward + intrinsic_reward\n",
    "        reward = np.clip(reward, -5, 5)\n",
    "        \n",
    "        # ========== MOVE TO NEXT WINDOW (CHRONOLOGICALLY) ==========\n",
    "        self.idx = (self.idx + 1) % len(self.X_train)\n",
    "        \n",
    "        # Next observation\n",
    "        next_emb = self.embeddings[self.idx]\n",
    "        next_err = np.array([self._ae_error(self.X_train[self.idx])])\n",
    "        obs = np.concatenate([next_emb, next_err]).astype(np.float32)\n",
    "        \n",
    "        done = self.steps >= self.max_steps\n",
    "        \n",
    "        return obs, reward, done, False, {}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Precompute embeddings\n",
    "# ============================================================================\n",
    "\n",
    "def compute_embeddings(X):\n",
    "    \"\"\"Compute embeddings for all windows.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        tensors = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "        embeds = ae_model.encode(tensors).cpu().numpy()\n",
    "    return embeds\n",
    "\n",
    "print(\"üîπ Precomputing embeddings for reconstructed data...\")\n",
    "embeddings_train = compute_embeddings(X_train_reconstructed)\n",
    "print(f\"‚úÖ Embeddings computed: {embeddings_train.shape}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Create VecEnv + QR-DQN\n",
    "# ============================================================================\n",
    "\n",
    "def make_env(rank):\n",
    "    def _init():\n",
    "        env = QRDQNAEEnvChronological(\n",
    "            X_train_reconstructed, \n",
    "            y_train_reconstructed, \n",
    "            supervision_mask,\n",
    "            ae_model,\n",
    "            embeddings_train,\n",
    "            lambda_int=0.8, \n",
    "            max_steps=2000\n",
    "        )\n",
    "        return Monitor(env, f\"logs/env_{rank}\")\n",
    "    return _init\n",
    "\n",
    "num_envs = 1\n",
    "vec_env = DummyVecEnv([make_env(i) for i in range(num_envs)])\n",
    "vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
    "\n",
    "# Policy kwargs (deeper for distributional modeling)\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[512, 256]  # QR-DQN benefits from slightly deeper nets\n",
    ")\n",
    "\n",
    "model = QRDQN(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    verbose=1,\n",
    "    learning_rate=1e-4,\n",
    "    buffer_size=150_000,         \n",
    "    learning_starts=5_000,\n",
    "    batch_size=64,\n",
    "    tau=1.0,\n",
    "    gamma=0.99,\n",
    "    train_freq=4,\n",
    "    target_update_interval=1_000,\n",
    "    exploration_fraction=0.1,\n",
    "    exploration_initial_eps=1.0,\n",
    "    exploration_final_eps=0.05,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    tensorboard_log=\"logs_qrdqn_ae_chronological/\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ Starting QR-DQN training with chronological data\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Train QR-DQN\n",
    "# ============================================================================\n",
    "\n",
    "model.learn(total_timesteps=500_000)  # More timesteps recommended for distributional methods\n",
    "model.save(\"qrdqn_ae_chronological\")\n",
    "vec_env.save(\"vec_normalize_qrdqn_chronological.pkl\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete! Chronological QR-DQN model saved.\")\n",
    "print(\"   Model: qrdqn_ae_chronological.zip\")\n",
    "print(\"   VecNormalize: vec_normalize_qrdqn_chronological.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf8123",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation for Quantile Regression DQN (QR-DQN)\n",
    "print(\"üìä Evaluating QR-DQN...\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "\n",
    "# Load test data\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")  # binary: 0 = normal, 1 = anomaly\n",
    "\n",
    "\n",
    "# Reuse observation creation function (same as training)\n",
    "def make_obs_array(X_windows):\n",
    "    obs_list = []\n",
    "    with torch.no_grad():\n",
    "        for x in X_windows:\n",
    "            t = torch.tensor(x, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            emb = ae_model.encode(t).cpu().numpy().flatten()\n",
    "            recon = ae_model(t)\n",
    "            loss = F.mse_loss(recon, t, reduction=\"mean\").item()\n",
    "            err = np.tanh((loss - ae_mean) / (ae_std + 1e-8))\n",
    "            err = float(max(0.0, err))\n",
    "            obs = np.concatenate([emb, [err]]).astype(np.float32)\n",
    "            obs_list.append(obs)\n",
    "    return np.array(obs_list)\n",
    "\n",
    "print(\"Preparing test observations...\")\n",
    "obs_test = make_obs_array(X_test)\n",
    "\n",
    "# === Inference for QR-DQN ===\n",
    "y_pred = []\n",
    "y_probs = []\n",
    "deterministic = True\n",
    "\n",
    "print(\"Running inference...\")\n",
    "for obs in obs_test:\n",
    "    obs_reshaped = obs.reshape(1, -1)\n",
    "\n",
    "    # Deterministic action (argmax over mean quantile)\n",
    "    action, _ = model.predict(obs_reshaped, deterministic=deterministic)\n",
    "    y_pred.append(int(action[0]))\n",
    "\n",
    "    # Get probability-like score for anomaly (action=1)\n",
    "    with torch.no_grad():\n",
    "        obs_tensor = torch.as_tensor(obs_reshaped, dtype=torch.float32).to(device)\n",
    "\n",
    "        # ‚îÄ‚îÄ‚îÄ Correct access for sb3-contrib QRDQN ‚îÄ‚îÄ‚îÄ\n",
    "        quantiles = model.policy.quantile_net(obs_tensor)   # [1, 200, 2]\n",
    "\n",
    "        # Average across quantiles ‚Üí approximate expected Q-value\n",
    "        q_mean = quantiles.mean(dim=1)                       # [1, 2]\n",
    "\n",
    "        probs = F.softmax(q_mean, dim=1)\n",
    "        prob_anomaly = probs[0, 1].item()                    # probability of anomaly (action 1)\n",
    "        y_probs.append(prob_anomaly)\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "y_probs = np.array(y_probs)\n",
    "\n",
    "# === Metrics ===\n",
    "cm = confusion_matrix(y_test, y_pred)  # [[TN, FP], [FN, TP]]\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "auc = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "# === Heatmap Confusion Matrix (Consistent Professional Style) ===\n",
    "cm_df = pd.DataFrame(\n",
    "    cm,\n",
    "    index=['True Normal (0)', 'True Anomaly (1)'],\n",
    "    columns=['Pred Normal (0)', 'Pred Anomaly (1)']\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', linewidths=1, linecolor='black',\n",
    "            cbar_kws={'label': 'Count'}, annot_kws={\"size\": 20, \"weight\": \"bold\"})\n",
    "plt.title('Confusion Matrix (QR-DQN)', fontsize=16, pad=20)\n",
    "plt.ylabel('True Label', fontsize=14)\n",
    "plt.xlabel('Predicted Label', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Print Metrics ===\n",
    "print(f\"\\nPrecision : {precision:.4f}\")\n",
    "print(f\"Recall    : {recall:.4f}\")\n",
    "print(f\"F1-Score  : {f1:.4f}\")\n",
    "print(f\"AUC       : {auc:.4f}\")\n",
    "print(f\"TP: {tp:,} | FP: {fp:,} | FN: {fn:,} | TN: {tn:,}\")\n",
    "\n",
    "# === ROC Curve ===\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.4f})', linewidth=3, color='#2E86AB')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', alpha=0.7)\n",
    "plt.xlabel('False Positive Rate', fontsize=14)\n",
    "plt.ylabel('True Positive Rate', fontsize=14)\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve (QR-DQN)', fontsize=16, pad=20)\n",
    "plt.legend(loc='lower right', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ QR-DQN Evaluation complete.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
